This folder shows three experiments from our latest research paper, "Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought" which are:

### [Experiment 1-Solidyfi-benchmark vulnerability detection capability test](https://github.com/Mirror-Tang/Evaluation-of-ChatGPT-s-Smart-Contract-Auditing-Capabilities-Based-on-Chain-of-Thought/blob/master/Experiment%201-Solidyfi-benchmark%20vulnerability%20detection%20capability%20test.md)

### [Experiment 2 - Code detection based on smart contract audit reports](https://github.com/Mirror-Tang/Evaluation-of-ChatGPT-s-Smart-Contract-Auditing-Capabilities-Based-on-Chain-of-Thought/blob/master/Experiment%202%20-%20Code%20detection%20based%20on%20smart%20contract%20audit%20reports.md)

### [Experiment 3 - Writing the PoC](https://github.com/Mirror-Tang/Evaluation-of-ChatGPT-s-Smart-Contract-Auditing-Capabilities-Based-on-Chain-of-Thought/blob/master/Experiment%203%20-%20Writing%20the%20POC.md)

Our study explores the potential of enhancing smart contract security audits using the GPT-4 model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using Chain of Thought (CoT) prompts based on the audit reports of eight groups of smart contracts. We also evaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs).

In order to facilitate the research of scholars, our experimental results are open to public.
